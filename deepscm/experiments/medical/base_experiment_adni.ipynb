{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/home/aay993/dscm/DSCM_implementation/')\n",
    "\n",
    "import pyro\n",
    "\n",
    "from pyro.nn import PyroModule, pyro_method\n",
    "\n",
    "from pyro.distributions import TransformedDistribution\n",
    "from pyro.infer.reparam.transform import TransformReparam\n",
    "from torch.distributions import Independent\n",
    "\n",
    "from deepscm.datasets.medical.adni import ADNIDataset\n",
    "from pyro.distributions.transforms import ComposeTransform, SigmoidTransform, AffineTransform\n",
    "\n",
    "import torchvision.utils\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_REGISTRY = {}\n",
    "MODEL_REGISTRY = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSEM(PyroModule):\n",
    "    def __init__(self, preprocessing: str = 'realnvp', downsample: int = -1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def _get_preprocess_transforms(self):\n",
    "        alpha = 0.05\n",
    "        num_bits = 8\n",
    "\n",
    "        if self.preprocessing == 'glow':\n",
    "            # Map to [-0.5,0.5]\n",
    "            a1 = AffineTransform(-0.5, (1. / 2 ** num_bits))\n",
    "            preprocess_transform = ComposeTransform([a1])\n",
    "        elif self.preprocessing == 'realnvp':\n",
    "            # Map to [0,1]\n",
    "            a1 = AffineTransform(0., (1. / 2 ** num_bits))\n",
    "\n",
    "            # Map into unconstrained space as done in RealNVP\n",
    "            a2 = AffineTransform(alpha, (1 - alpha))\n",
    "\n",
    "            s = SigmoidTransform()\n",
    "\n",
    "            preprocess_transform = ComposeTransform([a1, a2, s.inv])\n",
    "        \n",
    "        else: \n",
    "            raise ValueError(f'{self.preprocessing} is not valid.')\n",
    "\n",
    "        return preprocess_transform\n",
    "\n",
    "    @pyro_method\n",
    "    def pgm_model(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @pyro_method\n",
    "    def model(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @pyro_method\n",
    "    def pgm_scm(self, *args, **kwargs):\n",
    "        def config(msg):\n",
    "            if isinstance(msg['fn'], TransformedDistribution):\n",
    "                return TransformReparam() # Reparameterizer for transformed distribution latent variables. \n",
    "                # This is useful for transformed distributions with complex, geometry-changing transforms \n",
    "                # but where the posterior has simple shape in the space of the base_dist. \n",
    "                # this reparameterization only works for latent variables, not likelihoods. \n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        return pyro.poutine.reparam(self.pgm_model, config=config)(*args, **kwargs)\n",
    "\n",
    "    @pyro_method\n",
    "    def scm(self, *args, **kwargs):\n",
    "        def config(msg):\n",
    "            if isinstance(msg['fn'], TransformedDistribution):\n",
    "                return TransformReparam()\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        return pyro.poutine.reparam(self.model, config=config)(*args, **kwargs)\n",
    "\n",
    "    @pyro_method\n",
    "    def sample(self, n_samples=1):\n",
    "        with pyro.plate('observations', n_samples):\n",
    "            samples = self.model()\n",
    "\n",
    "        return (*samples,)\n",
    "\n",
    "    @pyro_method\n",
    "    def sample_scm(self, n_samples=1):\n",
    "        with pyro.plate('observations', n_samples):\n",
    "            samples = self.scm()\n",
    "\n",
    "        return (*samples,)\n",
    "\n",
    "    # Check if we need this later\n",
    "    @pyro_method\n",
    "    def infer_e_x(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @pyro_method\n",
    "    def infer_exogeneous(self, **obs):\n",
    "        # assuming that we use transformed distributions for everything:\n",
    "        cond_sample = pyro.condition(self.sample, data=obs)\n",
    "        batch_size = obs['x'].shape[0]\n",
    "        cond_trace = pyro.poutine.trace(cond_sample).get_trace(batch_size)\n",
    "\n",
    "        output = {}\n",
    "        for name, node in cond_trace.nodes.items():\n",
    "            if 'fn' not in node.keys():\n",
    "                continue\n",
    "\n",
    "            fn = node['fn']\n",
    "            if isinstance(fn, Independent):\n",
    "                fn = fn.base_dist\n",
    "            if isinstance(fn, TransformedDistribution):\n",
    "                # compute exogenous base distribution at all sites. base dist created with TransformReparam\n",
    "                output[name + '_base'] = ComposeTransform(fn.transforms).inv(node['value'])\n",
    "\n",
    "        return output\n",
    "\n",
    "    @pyro_method\n",
    "    def infer(self, **obs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @pyro_method\n",
    "    def counterfactual(self, obs, condition=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def add_arguments(cls, parser):\n",
    "        parser.add_argument('--preprocessing', default='realnvp', type=str, help=\"type of preprocessing (default: %(default)s)\", choices=['realnvp', 'glow'])\n",
    "        parser.add_argument('--downsample', default=-1, type=int, help=\"downsampling factor (default: %(default)s)\")\n",
    "\n",
    "        return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCovariateExperiment(pl.LightningModule):\n",
    "    def __init__(self, hparams, pyro_model: BaseSEM):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pyro_model = pyro_model\n",
    "        # Name of the experiment and pyro model we're using \n",
    "        hparams.experiment = self.__class__.__name__\n",
    "        hparams.model = pyro_model.__class__.__name__\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        self.train_batch_size = hparams.train_batch_size\n",
    "        self.test_batch_size = hparams.test_batch_size\n",
    "\n",
    "        # set the number of counterfactuals by creating a callable with pyro_model.counterfactual where the 'num_particles' argument is set.\n",
    "        if hasattr(hparams, 'num_sample_particles'):\n",
    "            self.pyro_model._gen_counterfactual = partial(self.pyro_model.counterfactual, num_particles=self.hparams.num_sample_particles)\n",
    "        else:\n",
    "            self.pyro_model._gen_counterfactual = self.pyro_model.counterfactual\n",
    "\n",
    "        if hparams.validate:\n",
    "            import random\n",
    "\n",
    "            torch.manual_seed(0)\n",
    "            np.random.seed(0)\n",
    "            random.seed(0)\n",
    "\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            torch.autograd.set_detect_anomaly(self.hparams.validate)\n",
    "            pyro.enable_validation()\n",
    "        \n",
    "        downsample = None if self.hparams.downsample == -1 else self.hparams.downsample\n",
    "        train_crop_type = self.hparams.train_crop_type if hasattr(self.hparams, 'train_crop_type') else 'random'\n",
    "        split_dir = self.hparams.split_dir if hasattr(self.hparams, 'split_dir') else '/home/aay993/'\n",
    "        data_dir = self.hparams.data_dir if hasattr(self.hparams, 'data_dir') else '/home/aay993/bias_corrected_registered_slices/'\n",
    "\n",
    "        # For now just using the same data for all three things \n",
    "        self.adni_train = ADNIDataset(f'{split_dir}/full_imputed_clinical_covariates.csv', base_path=data_dir, crop_type=train_crop_type, downsample=downsample)  # noqa: E501\n",
    "        self.adni_val = ADNIDataset(f'{split_dir}/full_imputed_clinical_covariates.csv', base_path=data_dir, crop_type='center', downsample=downsample)\n",
    "        self.adni_test = ADNIDataset(f'{split_dir}/full_imputed_clinical_covariates.csv', base_path=data_dir, crop_type='center', downsample=downsample)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.torch_device = self.trainer.root_gpu if self.trainer.on_gpu else self.trainer.root_device\n",
    "\n",
    "        # TODO: change ranges and decide what to condition on\n",
    "        brain_volumes = 800000. + 300000 * torch.arange(3, dtype=torch.float, device=self.torch_device)\n",
    "        self.brain_volume_range = brain_volumes.repeat(3).unsqueeze(1)\n",
    "        ventricle_volumes = 10000. + 50000 * torch.arange(3, dtype=torch.float, device=self.torch_device)\n",
    "        self.ventricle_volume_range = ventricle_volumes.repeat_interleave(3).unsqueeze(1)\n",
    "        self.z_range = torch.randn([1, self.hparams.latent_dim], device=self.torch_device, dtype=torch.float).repeat((9, 1))\n",
    "\n",
    "\n",
    "\n",
    "        self.pyro_model.age_flow_lognorm_loc = (self.adni_train.metrics['AGE'].log().mean().to(self.torch_device).float())\n",
    "        self.pyro_model.age_flow_lognorm_scale = (self.adni_train.metrics['AGE'].log().std().to(self.torch_device).float())\n",
    "\n",
    "        self.pyro_model.ventricle_volume_flow_lognorm_loc = (self.adni_train.metrics['Ventricles'].log().mean().to(self.torch_device).float())\n",
    "        self.pyro_model.ventricle_volume_flow_lognorm_scale = (self.adni_train.metrics['Ventricles'].log().std().to(self.torch_device).float())\n",
    "\n",
    "        self.pyro_model.brain_volume_flow_lognorm_loc = (self.adni_train.metrics['WholeBrain'].log().mean().to(self.torch_device).float())\n",
    "        self.pyro_model.brain_volume_flow_lognorm_scale = (self.adni_train.metrics['WholeBrain'].log().std().to(self.torch_device).float())\n",
    "\n",
    "        self.pyro_model.tau_flow_lognorm_loc = (self.adni_train.metrics['PTAU'].log().mean().to(self.torch_device).float())\n",
    "        self.pyro_model.tau_flow_lognorm_scale = (self.adni_train.metrics['PTAU'].log().std().to(self.torch_device).float())\n",
    "\n",
    "        self.pyro_model.av45_flow_lognorm_loc = (self.adni_train.metrics['AV45'].log().mean().to(self.torch_device).float())\n",
    "        self.pyro_model.av45_flow_lognorm_scale = (self.adni_train.metrics['AV45'].log().std().to(self.torch_device).float())\n",
    "\n",
    "        self.pyro_model.moca_flow_lognorm_loc = (self.adni_train.metrics['MOCA'].log().mean().to(self.torch_device).float())\n",
    "        self.pyro_model.moca_flow_lognorm_scale = (self.adni_train.metrics['MOCA'].log().std().to(self.torch_device).float()) \n",
    "\n",
    "        self.pyro_model.education_flow_lognorm_loc = (self.adni_train.metrics['PTEDUCAT'].log().mean().to(self.torch_device).float())\n",
    "        self.pyro_model.education_flow_lognorm_scale = (self.adni_train.metrics['PTEDUCAT'].log().std().to(self.torch_device).float())\n",
    "\n",
    "        self.pyro_model.apoE_min = torch.tensor([self.adni_train.metrics['APOE4'].min()-0.5-1e-3], device=self.torch_device, dtype=torch.float32)\n",
    "        self.pyro_model.apoE_max = torch.tensor([self.adni_train.metrics['APOE4'].max()+0.5+1e-3], device=self.torch_device, dtype=torch.float32)\n",
    "\n",
    "        self.pyro_model.slice_number_min = torch.tensor([self.adni_train.metrics['slice_number'].min()-0.5-1e-3], device=self.torch_device, dtype=torch.float32)\n",
    "        self.pyro_model.slice_number_max = torch.tensor([self.adni_train.metrics['slice_number'].max()-0.5-1e-3], device=self.torch_device, dtype=torch.float32)\n",
    "\n",
    "        if self.hparams.validate:\n",
    "            print(f'set ventricle_volume_flow_lognorm {self.pyro_model.ventricle_volume_flow_lognorm.loc} +/- {self.pyro_model.ventricle_volume_flow_lognorm.scale}')  # noqa: E501\n",
    "            print(f'set age_flow_lognorm {self.pyro_model.age_flow_lognorm.loc} +/- {self.pyro_model.age_flow_lognorm.scale}')\n",
    "            print(f'set brain_volume_flow_lognorm {self.pyro_model.brain_volume_flow_lognorm.loc} +/- {self.pyro_model.brain_volume_flow_lognorm.scale}')\n",
    "            print(f'set tau_flow_lognorm {self.pyro_model.tau_flow_lognorm.loc} +/- {self.pyro_model.tau_flow_lognorm.scale}')\n",
    "            print(f'set av45_flow_lognorm {self.pyro_model.av45_flow_lognorm.loc} +/- {self.pyro_model.av45_flow_lognorm.scale}')\n",
    "            print(f'set moca_flow_lognorm {self.pyro_model.moca_flow_lognorm.loc} +/- {self.pyro_model.moca_flow_lognorm.scale}')\n",
    "            print(f'set education_flow_lognorm {self.pyro_model.education_flow_lognorm.loc} +/- {self.pyro_model.education_flow_lognorm.scale}')\n",
    "            print(f'set ApoE min/max {int(self.pyro_model.apoE_min.item()):d} / {int(self.pyro_model.apoE_max.item()):d}')\n",
    "            print(f'set slice min/max {int(self.pyro_model.slice_number_min.item()):d} / {int(self.pyro_model.slice_number_max.item()):d}')\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        pass\n",
    "\n",
    "    def _dataloader_params(self):\n",
    "        num_workers = len(os.sched_getaffinity(0)) // 2  # use half of the available cpus\n",
    "        return {'num_workers': num_workers, 'pin_memory': self.trainer.on_gpu} # Note this is added from MS \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.adni_train, batch_size=self.train_batch_size, shuffle=True, **self._dataloader_params())\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        self.val_loader = DataLoader(self.adni_val, batch_size=self.test_batch_size, shuffle=False, **self._dataloader_params())\n",
    "        return self.val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        self.test_loader = DataLoader(self.adni_test, batch_size=self.test_batch_size, shuffle=False, **self._dataloader_params())\n",
    "        return self.test_loader\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def prep_batch(self, batch):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        outputs = self.assemble_epoch_end_outputs(outputs)\n",
    "\n",
    "        metrics = {('val/' + k): v for k, v in outputs.items()}\n",
    "\n",
    "        if self.current_epoch % self.hparams.sample_img_interval == 0:\n",
    "            self.sample_images()\n",
    "\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        print('Assembling outputs')\n",
    "        outputs = self.assemble_epoch_end_outputs(outputs)\n",
    "\n",
    "        samples = outputs.pop('samples')\n",
    "\n",
    "        sample_trace = pyro.poutine.trace(self.pyro_model.sample).get_trace(self.hparams.test_batch_size)\n",
    "        samples['unconditional_samples'] = {\n",
    "            'x': sample_trace.nodes['x']['value'].cpu(),\n",
    "            'brain_volume': sample_trace.nodes['brain_volume']['value'].cpu(),\n",
    "            'ventricle_volume': sample_trace.nodes['ventricle_volume']['value'].cpu(),\n",
    "            'age': sample_trace.nodes['age']['value'].cpu(),\n",
    "            'sex': sample_trace.nodes['sex']['value'].cpu()\n",
    "        }\n",
    "\n",
    "        cond_data = {'brain_volume': self.brain_volume_range, 'ventricle_volume': self.ventricle_volume_range, 'z': self.z_range}\n",
    "        cond_data = {\n",
    "            'brain_volume': self.brain_volume_range.repeat(self.hparams.test_batch_size, 1),\n",
    "            'ventricle_volume': self.ventricle_volume_range.repeat(self.hparams.test_batch_size, 1),\n",
    "            'z': torch.randn([self.hparams.test_batch_size, self.hparams.latent_dim], device=self.torch_device, dtype=torch.float).repeat_interleave(9, 0)\n",
    "        }\n",
    "        sample_trace = pyro.poutine.trace(pyro.condition(self.pyro_model.sample, data=cond_data)).get_trace(9 * self.hparams.test_batch_size)\n",
    "        samples['conditional_samples'] = {\n",
    "            'x': sample_trace.nodes['x']['value'].cpu(),\n",
    "            'brain_volume': sample_trace.nodes['brain_volume']['value'].cpu(),\n",
    "            'ventricle_volume': sample_trace.nodes['ventricle_volume']['value'].cpu(),\n",
    "            'age': sample_trace.nodes['age']['value'].cpu(),\n",
    "            'sex': sample_trace.nodes['sex']['value'].cpu()\n",
    "        }\n",
    "\n",
    "        print(f'Got samples: {tuple(samples.keys())}')\n",
    "\n",
    "        metrics = {('test/' + k): v for k, v in outputs.items()}\n",
    "\n",
    "        for k, v in samples.items():\n",
    "            p = os.path.join(self.trainer.logger.experiment.log_dir, f'{k}.pt')\n",
    "\n",
    "            print(f'Saving samples for {k} to {p}')\n",
    "\n",
    "            torch.save(v, p)\n",
    "\n",
    "        p = os.path.join(self.trainer.logger.experiment.log_dir, 'metrics.pt')\n",
    "        torch.save(metrics, p)\n",
    "\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "    def assemble_epoch_end_outputs(self, outputs):\n",
    "        num_items = len(outputs)\n",
    "\n",
    "        def handle_row(batch, assembled=None):\n",
    "            if assembled is None:\n",
    "                assembled = {}\n",
    "\n",
    "            for k, v in batch.items():\n",
    "                if k not in assembled.keys():\n",
    "                    if isinstance(v, dict):\n",
    "                        assembled[k] = handle_row(v)\n",
    "                    elif isinstance(v, float):\n",
    "                        assembled[k] = v\n",
    "                    elif np.prod(v.shape) == 1:\n",
    "                        assembled[k] = v.cpu()\n",
    "                    else:\n",
    "                        assembled[k] = v.cpu()\n",
    "                else:\n",
    "                    if isinstance(v, dict):\n",
    "                        assembled[k] = handle_row(v, assembled[k])\n",
    "                    elif isinstance(v, float):\n",
    "                        assembled[k] += v\n",
    "                    elif np.prod(v.shape) == 1:\n",
    "                        assembled[k] += v.cpu()\n",
    "                    else:\n",
    "                        assembled[k] = torch.cat([assembled[k], v.cpu()], 0)\n",
    "\n",
    "            return assembled\n",
    "\n",
    "        assembled = {}\n",
    "        for _, batch in enumerate(outputs):\n",
    "            assembled = handle_row(batch, assembled)\n",
    "\n",
    "        for k, v in assembled.items():\n",
    "            if (hasattr(v, 'shape') and np.prod(v.shape) == 1) or isinstance(v, float):\n",
    "                assembled[k] /= num_items\n",
    "\n",
    "        return assembled\n",
    "\n",
    "    def get_counterfactual_conditions(self, batch):\n",
    "        counterfactuals = {\n",
    "            'do(brain_volume=800000)': {'brain_volume': torch.ones_like(batch['brain_volume']) * 800000},\n",
    "            'do(brain_volume=1200000)': {'brain_volume': torch.ones_like(batch['brain_volume']) * 1200000},\n",
    "            'do(brain_volume=1600000)': {'brain_volume': torch.ones_like(batch['brain_volume']) * 1600000},\n",
    "            'do(ventricle_volume=10000)': {'ventricle_volume': torch.ones_like(batch['ventricle_volume']) * 10000},\n",
    "            'do(ventricle_volume=50000)': {'ventricle_volume': torch.ones_like(batch['ventricle_volume']) * 50000},\n",
    "            'do(ventricle_volume=110000)': {'ventricle_volume': torch.ones_like(batch['ventricle_volume']) * 110000},\n",
    "            'do(age=40)': {'age': torch.ones_like(batch['age']) * 40},\n",
    "            'do(age=80)': {'age': torch.ones_like(batch['age']) * 80},\n",
    "            'do(age=120)': {'age': torch.ones_like(batch['age']) * 120},\n",
    "            'do(sex=0)': {'sex': torch.zeros_like(batch['sex'])},\n",
    "            'do(sex=1)': {'sex': torch.ones_like(batch['sex'])},\n",
    "            'do(brain_volume=800000, ventricle_volume=224)': {'brain_volume': torch.ones_like(batch['brain_volume']) * 800000,\n",
    "                                                              'ventricle_volume': torch.ones_like(batch['ventricle_volume']) * 110000},\n",
    "            'do(brain_volume=1600000, ventricle_volume=10000)': {'brain_volume': torch.ones_like(batch['brain_volume']) * 1600000,\n",
    "                                                                 'ventricle_volume': torch.ones_like(batch['ventricle_volume']) * 10000}\n",
    "        }\n",
    "\n",
    "        return counterfactuals\n",
    "\n",
    "    def build_test_samples(self, batch):\n",
    "        samples = {}\n",
    "        samples['reconstruction'] = {'x': self.pyro_model.reconstruct(**batch, num_particles=self.hparams.num_sample_particles)}\n",
    "\n",
    "        counterfactuals = self.get_counterfactual_conditions(batch)\n",
    "\n",
    "        for name, condition in counterfactuals.items():\n",
    "            samples[name] = self.pyro_model._gen_counterfactual(obs=batch, condition=condition)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def log_img_grid(self, tag, imgs, normalize=True, save_img=False, **kwargs):\n",
    "        if save_img:\n",
    "            p = os.path.join(self.trainer.logger.experiment.log_dir, f'{tag}.png')\n",
    "            torchvision.utils.save_image(imgs, p)\n",
    "        grid = torchvision.utils.make_grid(imgs, normalize=normalize, **kwargs)\n",
    "        self.logger.experiment.add_image(tag, grid, self.current_epoch)\n",
    "\n",
    "    def get_batch(self, loader):\n",
    "        batch = next(iter(loader))\n",
    "        batch = {k: v.to(self.torch_device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()} #This might be more general-purpose \n",
    "        # if self.trainer.on_gpu:\n",
    "        #     batch = self.trainer.accelerator_backend.to_device(batch, self.torch_device)\n",
    "        return batch\n",
    "\n",
    "    def log_kdes(self, tag, data, save_img=False):\n",
    "        def np_val(x):\n",
    "            return x.cpu().numpy().squeeze() if isinstance(x, torch.Tensor) else x.squeeze()\n",
    "\n",
    "        fig, ax = plt.subplots(1, len(data), figsize=(5 * len(data), 3), sharex=True, sharey=True)\n",
    "        for i, (name, covariates) in enumerate(data.items()):\n",
    "            try:\n",
    "                if len(covariates) == 1:\n",
    "                    (x_n, x), = tuple(covariates.items())\n",
    "                    sns.kdeplot(x=np_val(x), ax=ax[i], shade=True, thresh=0.05)\n",
    "                elif len(covariates) == 2:\n",
    "                    (x_n, x), (y_n, y) = tuple(covariates.items())\n",
    "                    sns.kdeplot(x=np_val(x), y=np_val(y), ax=ax[i], shade=True, thresh=0.05)\n",
    "                    ax[i].set_ylabel(y_n)\n",
    "                else:\n",
    "                    raise ValueError(f'got too many values: {len(covariates)}')\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(f'got a linalg error when plotting {tag}/{name}')\n",
    "\n",
    "            ax[i].set_title(name)\n",
    "            ax[i].set_xlabel(x_n)\n",
    "\n",
    "        sns.despine()\n",
    "\n",
    "        if save_img:\n",
    "            p = os.path.join(self.trainer.logger.experiment.log_dir, f'{tag}.png')\n",
    "            plt.savefig(p, dpi=300)\n",
    "\n",
    "        self.logger.experiment.add_figure(tag, fig, self.current_epoch)\n",
    "\n",
    "    def build_reconstruction(self, x, age, sex, ventricle_volume, brain_volume, tag='reconstruction'):\n",
    "        obs = {'x': x, 'age': age, 'sex': sex, 'ventricle_volume': ventricle_volume, 'brain_volume': brain_volume}\n",
    "\n",
    "        recon = self.pyro_model.reconstruct(**obs, num_particles=self.hparams.num_sample_particles)\n",
    "        self.log_img_grid(tag, torch.cat([x, recon], 0))\n",
    "        self.logger.experiment.add_scalar(f'{tag}/mse', torch.mean(torch.square(x - recon).sum((1, 2, 3))), self.current_epoch)\n",
    "\n",
    "    def build_counterfactual(self, tag, obs, conditions, absolute=None):\n",
    "        _required_data = ('x', 'age', 'sex', 'ventricle_volume', 'brain_volume')\n",
    "        assert set(obs.keys()) == set(_required_data), 'got: {}'.format(tuple(obs.keys()))\n",
    "\n",
    "        imgs = [obs['x']]\n",
    "        # TODO: decide which kde's to plot in which configuration\n",
    "        if absolute == 'brain_volume':\n",
    "            sampled_kdes = {'orig': {'ventricle_volume': obs['ventricle_volume']}}\n",
    "        elif absolute == 'ventricle_volume':\n",
    "            sampled_kdes = {'orig': {'brain_volume': obs['brain_volume']}}\n",
    "        else:\n",
    "            sampled_kdes = {'orig': {'brain_volume': obs['brain_volume'], 'ventricle_volume': obs['ventricle_volume']}}\n",
    "\n",
    "        for name, data in conditions.items():\n",
    "            counterfactual = self.pyro_model._gen_counterfactual(obs=obs, condition=data)\n",
    "\n",
    "            counter = counterfactual['x']\n",
    "            sampled_brain_volume = counterfactual['brain_volume']\n",
    "            sampled_ventricle_volume = counterfactual['ventricle_volume']\n",
    "\n",
    "            imgs.append(counter)\n",
    "            if absolute == 'brain_volume':\n",
    "                sampled_kdes[name] = {'ventricle_volume': sampled_ventricle_volume}\n",
    "            elif absolute == 'ventricle_volume':\n",
    "                sampled_kdes[name] = {'brain_volume': sampled_brain_volume}\n",
    "            else:\n",
    "                sampled_kdes[name] = {'brain_volume': sampled_brain_volume, 'ventricle_volume': sampled_ventricle_volume}\n",
    "\n",
    "        self.log_img_grid(tag, torch.cat(imgs, 0))\n",
    "        self.log_kdes(f'{tag}_sampled', sampled_kdes, save_img=True)\n",
    "\n",
    "    def sample_images(self):\n",
    "        with torch.no_grad():\n",
    "            # TODO: redo all this....\n",
    "            sample_trace = pyro.poutine.trace(self.pyro_model.sample).get_trace(self.hparams.test_batch_size)\n",
    "\n",
    "            samples = sample_trace.nodes['x']['value']\n",
    "            sampled_brain_volume = sample_trace.nodes['brain_volume']['value']\n",
    "            sampled_ventricle_volume = sample_trace.nodes['ventricle_volume']['value']\n",
    "\n",
    "            self.log_img_grid('samples', samples.data[:8])\n",
    "\n",
    "            cond_data = {'brain_volume': self.brain_volume_range, 'ventricle_volume': self.ventricle_volume_range, 'z': self.z_range}\n",
    "            samples, *_ = pyro.condition(self.pyro_model.sample, data=cond_data)(9)\n",
    "            self.log_img_grid('cond_samples', samples.data, nrow=3)\n",
    "\n",
    "            obs_batch = self.prep_batch(self.get_batch(self.val_loader))\n",
    "\n",
    "            kde_data = {\n",
    "                'batch': {'brain_volume': obs_batch['brain_volume'], 'ventricle_volume': obs_batch['ventricle_volume']},\n",
    "                'sampled': {'brain_volume': sampled_brain_volume, 'ventricle_volume': sampled_ventricle_volume}\n",
    "            }\n",
    "            self.log_kdes('sample_kde', kde_data, save_img=True)\n",
    "\n",
    "            exogeneous = self.pyro_model.infer(**obs_batch)\n",
    "\n",
    "            for (tag, val) in exogeneous.items():\n",
    "                self.logger.experiment.add_histogram(tag, val, self.current_epoch)\n",
    "\n",
    "            obs_batch = {k: v[:8] for k, v in obs_batch.items()}\n",
    "\n",
    "            self.log_img_grid('input', obs_batch['x'], save_img=True)\n",
    "\n",
    "            if hasattr(self.pyro_model, 'reconstruct'):\n",
    "                self.build_reconstruction(**obs_batch)\n",
    "\n",
    "            conditions = {\n",
    "                '40': {'age': torch.zeros_like(obs_batch['age']) + 40},\n",
    "                '60': {'age': torch.zeros_like(obs_batch['age']) + 60},\n",
    "                '80': {'age': torch.zeros_like(obs_batch['age']) + 80}\n",
    "            }\n",
    "            self.build_counterfactual('do(age=x)', obs=obs_batch, conditions=conditions)\n",
    "\n",
    "            conditions = {\n",
    "                '0': {'sex': torch.zeros_like(obs_batch['sex'])},\n",
    "                '1': {'sex': torch.ones_like(obs_batch['sex'])},\n",
    "            }\n",
    "            self.build_counterfactual('do(sex=x)', obs=obs_batch, conditions=conditions)\n",
    "\n",
    "            conditions = {\n",
    "                '800000': {'brain_volume': torch.zeros_like(obs_batch['brain_volume']) + 800000},\n",
    "                '1100000': {'brain_volume': torch.zeros_like(obs_batch['brain_volume']) + 1100000},\n",
    "                '1400000': {'brain_volume': torch.zeros_like(obs_batch['brain_volume']) + 1400000},\n",
    "                '1600000': {'brain_volume': torch.zeros_like(obs_batch['brain_volume']) + 1600000}\n",
    "            }\n",
    "            self.build_counterfactual('do(brain_volume=x)', obs=obs_batch, conditions=conditions, absolute='brain_volume')\n",
    "\n",
    "            conditions = {\n",
    "                '10000': {'ventricle_volume': torch.zeros_like(obs_batch['ventricle_volume']) + 10000},\n",
    "                '25000': {'ventricle_volume': torch.zeros_like(obs_batch['ventricle_volume']) + 25000},\n",
    "                '50000': {'ventricle_volume': torch.zeros_like(obs_batch['ventricle_volume']) + 50000},\n",
    "                '75000': {'ventricle_volume': torch.zeros_like(obs_batch['ventricle_volume']) + 75000},\n",
    "                '110000': {'ventricle_volume': torch.zeros_like(obs_batch['ventricle_volume']) + 110000}\n",
    "            }\n",
    "            self.build_counterfactual('do(ventricle_volume=x)', obs=obs_batch, conditions=conditions, absolute='ventricle_volume')\n",
    "\n",
    "    @classmethod\n",
    "    def add_arguments(cls, parser):\n",
    "        parser.add_argument('--data_dir', default=\"/vol/biomedic2/bglocker/gemini/UKBB/t0/\", type=str, help=\"data dir (default: %(default)s)\")  # noqa: E501\n",
    "        parser.add_argument('--split_dir', default=\"/vol/biomedic2/np716/data/gemini/ukbb/ventricle_brain/\", type=str, help=\"split dir (default: %(default)s)\")  # noqa: E501\n",
    "        parser.add_argument('--sample_img_interval', default=10, type=int, help=\"interval in which to sample and log images (default: %(default)s)\")\n",
    "        parser.add_argument('--train_batch_size', default=64, type=int, help=\"train batch size (default: %(default)s)\")\n",
    "        parser.add_argument('--test_batch_size', default=64, type=int, help=\"test batch size (default: %(default)s)\")\n",
    "        parser.add_argument('--validate', default=False, action='store_true', help=\"whether to validate (default: %(default)s)\")\n",
    "        parser.add_argument('--lr', default=1e-4, type=float, help=\"lr of deep part (default: %(default)s)\")\n",
    "        parser.add_argument('--pgm_lr', default=5e-3, type=float, help=\"lr of pgm (default: %(default)s)\")\n",
    "        parser.add_argument('--l2', default=0., type=float, help=\"weight decay (default: %(default)s)\")\n",
    "        parser.add_argument('--use_amsgrad', default=False, action='store_true', help=\"use amsgrad? (default: %(default)s)\")\n",
    "        parser.add_argument('--train_crop_type', default='random', choices=['random', 'center'], help=\"how to crop training images (default: %(default)s)\")\n",
    "\n",
    "        return parser"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('dscm': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "142ccf925348e6e849eb5106808af40cbad73a6d9b4e62251197a482f88424bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
